NLP

# Web scraping, pickle imports
import requests
from bs4 import BeautifulSoup
import pickle

# Scrapes transcript data from scrapsfromtheloft.com
def url_to_transcript(url):
    '''Returns transcript data specifically from scrapsfromtheloft.com.'''
    page = requests.get(url).text
    soup = BeautifulSoup(page, "lxml")
    text = [p.text for p in soup.find_all(class_="site-content",)]
    print(url)
    return text


# URLs of transcripts in scope
urls = ['https://scrapsfromtheloft.com/comedy/david-nihill-cultural-appreciation-transcript/',
        'https://scrapsfromtheloft.com/comedy/lewis-black-tragically-i-need-you-transcript/',
        'https://scrapsfromtheloft.com/comedy/george-carlin-im-glad-im-dead-transcript/',
        'https://scrapsfromtheloft.com/comedy/mike-birbiglia-old-man-and-pool-transcript/',
        'https://scrapsfromtheloft.com/comedy/gary-gulman-born-on-3rd-base-transcript/',
        'https://scrapsfromtheloft.com/comedy/beth-stelling-if-you-didnt-want-me-then-transcript/',
        'https://scrapsfromtheloft.com/comedy/michelle-wolf-its-great-to-be-here-transcript/',
        'https://scrapsfromtheloft.com/comedy/mark-normand-soup-to-nuts-transcript/',
        'https://scrapsfromtheloft.com/comedy/jared-freid-37-and-single-transcript/',
        'https://scrapsfromtheloft.com/comedy/tom-segura-sledgehammer-transcript/',
       ]

# Comedian names
comedians = ['david', 'lewis', 'george', 'mike', 'gary', 'beth', 'michelle', 'mark', 'jared', 'tom']

# # Actually request transcripts (takes a few minutes to run)
transcripts = [url_to_transcript(u) for u in urls]

# # Pickle files for later use

# # Make a new directory to hold the text files
!mkdir transcripts

for i, c in enumerate(comedians):
  with open("transcripts/" + c + ".txt", "wb") as file:
      pickle.dump(transcripts[i], file)

# Load pickled files
data = {}
for i, c in enumerate(comedians):
    with open("transcripts/" + c + ".txt", "rb") as file:
        data[c] = pickle.load(file)

# Double check to make sure data has been loaded properly
data.keys()

# More checks
data['david'][:2]

Cleaning The Data
When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.

With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate.

Assignment:
Perform the following data cleaning on transcripts: i) Make text all lower case ii) Remove punctuation iii) Remove numerical values iv) Remove common non-sensical text (/n) v) Tokenize text vi) Remove stop words

# Let's take a look at our data again
next(iter(data.keys()))

# Notice that our dictionary is currently in key: comedian, value: list of text format
next(iter(data.values()))

# We are going to change this to key: comedian, value: string format
def combine_text(list_of_text):
    '''Takes a list of text and combines them into one large chunk of text.'''
    combined_text = ' '.join(list_of_text)
    return combined_text

# Combine it!
data_combined = {key: [combine_text(value)] for (key, value) in data.items()}

# We can either keep it in dictionary format or put it into a pandas dataframe
import pandas as pd
pd.set_option('max_colwidth',150)

data_df = pd.DataFrame.from_dict(data_combined).transpose()
data_df.columns = ['transcript']
data_df = data_df.sort_index()
data_df

# Let's take a look at the transcript for Ali Wong
data_df.transcript.loc['jared']

# Apply a first round of text cleaning techniques
import re
import string

def clean_text_round1(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

round1 = lambda x: clean_text_round1(x)

# Let's take a look at the updated text
data_clean = pd.DataFrame(data_df.transcript.apply(round1))
data_clean

# Apply a second round of cleaning
def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub('\n', '', text)
    return text

round2 = lambda x: clean_text_round2(x)

# Let's take a look at the updated text
data_clean = pd.DataFrame(data_clean.transcript.apply(round2))
data_clean

Organizing The Data
Assignment:
Organized data in two standard text formats: a) Corpus - corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here. b) Document-Term Matrix - word counts in matrix format

Corpus: Example
A corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here.

# Let's take a look at our dataframe
data_df

# Let's add the comedians' full names as well
full_names = ['David Nihill', 'Lewis Black', 'George Carlin', 'Mike Birbiglia', 'Gary Gulman', 'Beth Stelling', 'Michelle Wolf', 'Mark Normand', 'Jared Fried', 'Tom Segura']

data_df['full_name'] = full_names
data_df

# Let's pickle it for later use
data_df.to_pickle("corpus.pkl")

Document-Term Matrix: Example
For many of the techniques we'll be using in future assignment, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.

In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc.

# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words='english')
data_cv = cv.fit_transform(data_clean.transcript)
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())
data_dtm.index = data_clean.index
data_dtm

# Let's pickle it for later use
data_dtm.to_pickle("dtm.pkl")

# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object
data_clean.to_pickle('data_clean.pkl')
pickle.dump(cv, open("cv.pkl", "wb"))

Additional Assignments:
Can you add an additional regular expression to the clean_text_round2 function to further clean the text?
Play around with CountVectorizer's parameters. What is ngram_range? What is min_df and max_df?

def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub('\n', '', text)
    text = re.sub(r'\b(?:haha|hehe|hoho)\b', '', text, flags=re.IGNORECASE)  # Remove laughter mentions
    text = re.sub(r'\b\d+\b', '', text)  # Remove numbers
    text = re.sub('[^a-zA-Z\s]', '', text)  # Remove special characters
    return text

round2 = lambda x: clean_text_round2(x)
data_clean = pd.DataFrame(data_clean.transcript.apply(round2))


from sklearn.feature_extraction.text import CountVectorizer

# Define parameters for CountVectorizer
ngram_range = (1, 2)  # Unigrams and bigrams
min_df = 5  # Minimum document frequency for a term to be included
max_df = 0.6  # Maximum document frequency for a term to be included

# Create a custom CountVectorizer
custom_cv = CountVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df)

# Fit and transform the text data using the custom CountVectorizer
data_cv_custom = custom_cv.fit_transform(data_clean.transcript)

# Create a DataFrame with the document-term matrix (DTM)
data_dtm_custom = pd.DataFrame(data_cv_custom.toarray(), columns=custom_cv.get_feature_names_out())
data_dtm_custom.index = data_clean.index

# Display the resulting document-term matrix
data_dtm_custom

Exploratory Data Analysis
Introduction
After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.

When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:

Most common words - find these and create word clouds
Size of vocabulary - look number of unique words and also how quickly someone speaks
Amount of profanity - most common terms

Most Common Words
Analysis

# task 1
import pickle
from collections import Counter,defaultdict
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk import FreqDist
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
import string
#from profanity_check import predict
nltk.download('stopwords')

with open('dtm.pkl', 'rb') as file:
    comedian_data = pickle.load(file)
comedian_names = comedian_data.index
comedian_word_counts = comedian_data.iloc[:, :]

comedian_durations = {'david': 60, 'lewis': 45, 'george': 55, 'mike': 40, 'gary': 50, 'beth': 60, 'michelle': 45, 'mark': 50, 'jared': 30, 'tom': 55}
comedian_data['total_words'] = 0
comedian_data['run_times'] = 0
comedian_data['words_per_minute'] = 0.0

for comedian_name in comedian_names:
    word_counts = comedian_word_counts.loc[comedian_name, :]

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Most Common Words for {comedian_name}\'s Comedy')
    plt.show()


import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Task 2
for comedian_name, comedian_words in zip(comedian_names, comedian_word_counts.values):
    all_text = ' '.join([word * int(count) for word, count in zip(comedian_word_counts.columns, comedian_words)]).lower()
    words = word_tokenize(all_text)
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]

    total_words = len(filtered_words)

    if comedian_name.lower() in comedian_durations:
        duration = comedian_durations[comedian_name.lower()]
        words_per_minute = total_words / duration
    else:
        words_per_minute = 0.0

    comedian_data.at[comedian_name, 'total_words'] = total_words
    comedian_data.at[comedian_name, 'run_times'] = duration
    comedian_data.at[comedian_name, 'words_per_minute'] = words_per_minute

print(comedian_data[['total_words', 'run_times', 'words_per_minute']])

all_text = ' '.join([' '.join([str(word) * int(count) for word, count in zip(comedian_word_counts.columns, comedian_words)]) for comedian_words in comedian_word_counts.values]).lower()
words = word_tokenize(all_text)

stop_words = set(stopwords.words('english'))

filtered_words = [word for word in words if word.lower() not in stop_words]

top_words_count = 10
most_common_words = Counter(filtered_words).most_common(top_words_count)

print(f"Top {top_words_count} most common words and their counts:")

for word, count in most_common_words:
    print(f"{word}: {count}")
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(most_common_words))

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Common Words Across Comedians')
plt.show()


NOTE: At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.

Find and print the top 30 words said by each comedian

comedian_data = pd.DataFrame(comedian_data)
comedian_names = comedian_data.index
comedian_word_counts = comedian_data.iloc[:, :]
def get_top_words(comedian_words, top_count=30):
    all_text = ' '.join([word * int(comedian_words[word]) for word in comedian_word_counts.columns]).lower()
    words = word_tokenize(all_text)
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]
    top_words = Counter(filtered_words).most_common(top_count)

    return top_words
for comedian_name in comedian_names:
    comedian_words = comedian_word_counts.loc[comedian_name, :]
    top_words = get_top_words(comedian_words)

    print(f"\nTop 30 words for {comedian_name}:")
    for word, count in top_words:
        print(f"{word}")

By looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that. Look at the most common top words and add them to the stop word list.

def add_stop_words(comedian_words, top_count=30):
    all_text = ' '.join([word * int(comedian_words[word]) for word in comedian_word_counts.columns]).lower()

    words = word_tokenize(all_text)

    stop_words = set(stopwords.words('english'))

    top_words = Counter(words).most_common(top_count)

    stop_words.update([word for word, _ in top_words])

    return top_words

stop_words = set(stopwords.words('english'))
for comedian_name in comedian_names:
    comedian_words = comedian_word_counts.loc[comedian_name, :]
    top_words = add_stop_words(comedian_words)

print("\nUpdated stop words list:")
print(stop_words)

Let's aggregate this list and identify the most common words along with how many routines they occur in

# Let's aggregate this list and identify the most common words along with how many routines they occur in
Counter(words).most_common()


# If more than half of the comedians have it as a top word, exclude it from the list
add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]
add_stop_words

import pickle
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

with open('data_clean.pkl', 'rb') as file:
    cleaned_data = pickle.load(file)
print("Column Names:", cleaned_data.columns)

def custom_analyzer(text):
    words = word_tokenize(text)
    return [word for word in words if word.lower() not in stop_words]

vectorizer = CountVectorizer(analyzer=custom_analyzer)
dtm = vectorizer.fit_transform(cleaned_data['transcript'])

with open('document_term_matrix.pkl', 'wb') as file:
    pickle.dump(dtm, file)

with open('vectorizer.pkl', 'wb') as file:
    pickle.dump(vectorizer, file)

Assignment 2:
Find the number of unique words that each comedian uses.

import pickle
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

with open('dtm.pkl', 'rb') as file:
    comedian_data = pickle.load(file)
comedian_names = comedian_data.index
comedian_word_counts = comedian_data.iloc[:, 1:]

for comedian_name, comedian_words in zip(comedian_names, comedian_word_counts.values):
    all_text = ' '.join([word * count for word, count in zip(comedian_word_counts.columns, comedian_words)]).lower()

    words = word_tokenize(all_text)

    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]

    unique_words = set(filtered_words)
    num_unique_words = len(unique_words)

    print(f"{comedian_name} uses {num_unique_words} unique words.")


Calculate the words per minute of each comedian

# Calculate the words per minute of each comedian

print("Calculate the words per minute of each comedian")
comedian_durations = {'david': 60, 'lewis': 45, 'george': 55, 'mike': 40, 'gary': 50, 'beth': 60, 'michelle': 45, 'mark': 50, 'jared': 30, 'tom': 55}

for comedian_name, comedian_words in zip(comedian_names, comedian_word_counts.values):
    all_text = ' '.join([word * count for word, count in zip(comedian_word_counts.columns, comedian_words)]).lower()

    # Tokenize the text
    words = word_tokenize(all_text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]

    # Calculate the total number of words
    total_words = len(filtered_words)

    # Calculate words per minute
    # if comedian_name.lower() in comedian_durations:
    #     duration = comedian_durations[comedian_name.lower()]
    #     wpm = total_words / duration
    #     print(f"{comedian_name} has {wpm:.2f} words per minute.")
    # else:
    #     print(f"{comedian_name} has no duration information.")




# Find the total number of words that a comedian uses
print()
print()
print("Find the total number of words that a comedian uses")
total_words_per_comedian = comedian_word_counts.sum(axis=1)

result_df = pd.DataFrame({ 'Total Words': total_words_per_comedian})
# print(result_df)



# add some columns to our dataframe: 'total_words', 'run_times' and 'words_per_minute'
print()
print()
print()
print("add some columns to our dataframe: 'total_words', 'run_times' and 'words_per_minute'")
# Create new columns
comedian_data['total_words'] = 0
comedian_data['run_times'] = 0
comedian_data['words_per_minute'] = 0.0
for comedian_name, comedian_words in zip(comedian_names, comedian_word_counts.values):
    # Combine all text data into a single string
    all_text = ' '.join([word * count for word, count in zip(comedian_word_counts.columns, comedian_words)]).lower()

    # Tokenize the text
    words = word_tokenize(all_text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]

    # Calculate the total number of words
    total_words = len(filtered_words)

    # Calculate words per minute
    if comedian_name.lower() in comedian_durations:
        duration = comedian_durations[comedian_name.lower()]
        words_per_minute = total_words / duration
    else:
        words_per_minute = 0.0

    # Update DataFrame with new values
    comedian_data.at[comedian_name, 'total_words'] = total_words
    comedian_data.at[comedian_name, 'run_times'] = duration
    comedian_data.at[comedian_name, 'words_per_minute'] = words_per_minute

# Display the updated DataFrame
# print(comedian_data)

# Sort the dataframe by words per minute to see who talks the slowest and fastest
print()
print()
print()
print("Sort the dataframe by words per minute to see who talks the slowest and fastest")
comedian_data_sorted = comedian_data.sort_values(by='words_per_minute', ascending=False)
print(comedian_data_sorted)

plot our findings

# plot our findings
plt.figure(figsize=(12, 6))
plt.bar(comedian_data_sorted.index, comedian_data_sorted['words_per_minute'], color='skyblue')
plt.title('Words Per Minute by Comedian')
plt.xlabel('Comedian')
plt.ylabel('Words Per Minute')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

Write your findings. For example:
Talking Speed
Mike Birbiglia has the highest words per minute at approximately 60, while George Carlin has the lowest at around 20.
Most comedians speak at over 40 words per minute, with some variability.
Assignment 3:
Check the profanity by analysing the common bad words, like fucking, fuck, `shit etc.

with open('dtm.pkl', 'rb') as file:
    comedian_data = pickle.load(file)
comedian_names = comedian_data.index
comedian_word_counts = comedian_data.iloc[:, :]
# Define profane words
profane_words = ['fuck', 'fucking', 'shit', 'damn', 'bitch', 'ass']

# Calculate total profanity occurrences for each comedian
comedian_data['total_profanity'] = comedian_data[profane_words].sum(axis=1)

# Display the total profanity occurrences
print("\nTotal Profanity Occurrences:")
print(comedian_data['total_profanity'])

# Plot a scatter plot of total profanity occurrences
plt.figure(figsize=(10, 6))
plt.scatter(comedian_data.index, comedian_data['total_profanity'], color='red')
plt.title('Total Profanity Occurrences for Each Comedian')
plt.xlabel('Comedian Names')
plt.ylabel('Total Profanity Occurrences')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

Write your findings.
Comedian with the most profanities - Lewis Black has the highest number of profanity occurrences, with 149.
Comedian with the least profanities - Mike Birbiglia has the lowest number of profanity occurrences, with only 1.Co
Comedians with moderate profanity use - B George Carlin, Jared Fried, Mark Normand all have moderate profanity occurrences.


Assignment 4:(optional)
What other word counts do you think would be interesting to compare instead of the f-word and s-word? Create a scatter plot comparing them.

